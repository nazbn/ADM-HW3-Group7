{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 ADM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from html.parser import HTMLParser\n",
    "import codecs\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from lxml import etree\n",
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this homework, we will see how to download webpages and scrape them. After having done that, we will show how we implemented search engines to match queries with vocabulary extracted (and \"cleaned\") from the valid webpages.\n",
    "\n",
    "Please note that, to present a readable jupyter file, we will often show the code applied to one of the three folders. Still, you will be able to find the entire code written for each folder on the python files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first open the html file thanks to the codecs library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 =open(\"C:/Users/chari/HW3ADMGR12/movies1.html\", 'r')\n",
    "link2 =open(\"C:/Users/chari/HW3ADMGR12/movies2.html\", 'r')\n",
    "link3 =open(\"C:/Users/chari/HW3ADMGR12/movies3.html\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.d = []\n",
    "        self.info = [] # This list will be useful to get the info from the infobox of wikipedia\n",
    "        self.tag = [] # We will use that to know when we have to stop retrieving the paragraphs\n",
    "        super().__init__()\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.tag = []\n",
    "        self.tag.append(tag)\n",
    "        return(tag)\n",
    "        \n",
    "    def handle_data(self, data):\n",
    "        if data.startswith('https'):\n",
    "            self.d.append(data)\n",
    "        else:\n",
    "            self.info.append(data)\n",
    "        return(data)\n",
    "\n",
    "    def return_data(self):\n",
    "        return(self.d)\n",
    "    \n",
    "    def return_info(self):\n",
    "        return(self.info)\n",
    "    \n",
    "    def return_tag(self):\n",
    "        if len(self.tag)!= 0:\n",
    "            return(self.tag[0])\n",
    "    \n",
    "# To check if it is an url, we check if it begins with \"https\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now simply add all the urls to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls1 = []\n",
    "urls2 = []\n",
    "urls3 = []\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "for item in list(link1):\n",
    "    parser.feed(item)\n",
    "    urls1 += parser.return_data()\n",
    "parser.close()\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "for item in list(link2):\n",
    "    parser.feed(item)\n",
    "    urls2 += parser.return_data()\n",
    "parser.close()\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "for item in list(link3):\n",
    "    parser.feed(item)\n",
    "    urls3 += parser.return_data()\n",
    "parser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(urls1),len(urls2),len(urls3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating three different parsers, we decided to create the same one three times, by closing it after the creation of each one of the three lists of URLs.\n",
    "\n",
    "We can also see that we got the same amount of URLs in each folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now move on to the 2nd part of the question\n",
    "\n",
    "We commented the code because we already retrieved the webpages and we don't need to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,len(urls1)+1):\n",
    "    try:\n",
    "        name = 'C:/Users/chari/HW3ADMGR12/movie1/article'+str(k)+'.html'\n",
    "        urllib.request.urlretrieve(urls3[k-1], name)\n",
    "    except urllib.error.HTTPError as err:\n",
    "        print(err.code)\n",
    "    except Exception :\n",
    "        print(\"error\")\n",
    "        time.sleep(1200)\n",
    "        urllib.request.urlretrieve(urls3[k-1], name)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided not to put the line \"urllib.request.urlretrieve(urls1[k-1], name)\" also after the 20 minute break because we haven't faced the exception of Wikipedia not allowing us to download the page. The only exceptio\n",
    "\n",
    "Still, we saw that two webpages were missing (9998 pages from the folder movies1). We decided to get the indices of the missing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having tried to add them manually, we saw that the library urllib constantly raised a 404 error, even though the URLs were valid. By looking closer to these two URLs, we can see that wikipedia does not find wikipedia pages on these films. Hence, it is normal that we were not able to retrieve these two files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now get the tsv files :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is an important thing to note : in order no to take a potential paragraph in the infobox as the 1st section, we added a condition in order not to consider this paragraph. That's why we only consider paragraphs from outside the tables.**\n",
    "\n",
    "Moreover, we must not consider the cases where the **first** paragraph met in a new section is just a backline, as we want text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = [\"Directed by\",\"Produced by\",\"Written by\",\"Starring\",\"Music by\",\"Release date\",\"Running time\",\"Country\",\"Language\",\"Budget\"]\n",
    "\n",
    "for k in range(1,10001):\n",
    "    \n",
    "    tot = []\n",
    "    A = []\n",
    "    B = []\n",
    "    name = \"C:/Users/chari/HW3ADMGR12/movie1/article_\" + str(k) + \".html\"\n",
    "    try:\n",
    "        file = open(name, encoding='utf8')\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        # We have to check if the page refers to a film or is a disambiguous page. We do not want to save the disambiguous ones.\n",
    "        disambiguous = soup.find(\"table\", {\"id\" : \"disambigbox\"})\n",
    "        \n",
    "        if disambiguous is not None :\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        right_table = soup.find(\"table\", attrs={\"class\":\"infobox vevent\"})\n",
    "\n",
    "        # We get the title of the film\n",
    "\n",
    "        # We put a condition to ONLY retrieve the title of the film, without the parenthesis containing the word \"movie\" and the year\n",
    "\n",
    "        if soup.select_one('#firstHeading').find(\"i\") != None:\n",
    "\n",
    "            title = soup.select_one('#firstHeading').find(\"i\").text\n",
    "\n",
    "        else:\n",
    "\n",
    "            title  = soup.select_one('#firstHeading').text   \n",
    "\n",
    "        tot.append(title)\n",
    "\n",
    "        # We get the intro and the plot (1st and 2nd sections)\n",
    "\n",
    "        intro = ''\n",
    "        plot = ''\n",
    "\n",
    "\n",
    "        # We get the intro first\n",
    "\n",
    "        tag  = soup.select_one('p')\n",
    "\n",
    "        if tag != None:\n",
    "\n",
    "            if tag.find_parent('table') == None and tag.text!=\"\\n\": # We get the intro if it is not a simple \"\\n\".\n",
    "                while tag.name == 'p':\n",
    "                    intro+=tag.text\n",
    "                    tag = tag.find_next_sibling()\n",
    "\n",
    "\n",
    "            else: # If the paragraph is in the infobox, we don't append the text but look for the next section\n",
    "\n",
    "                while tag.text == \"\\n\" or tag.find_parent('table') != None :\n",
    "                    tag = tag.find_next(\"p\")\n",
    "\n",
    "\n",
    "                while tag.name == 'p': # While we encounter <p> tags, we add them to the intro. Once a different tag is found, it would mean that we reached the end of the intro\n",
    "                    intro+=tag.text\n",
    "                    tag = tag.find_next_sibling()\n",
    "\n",
    "\n",
    "            # We can now retrieve the second section\n",
    "\n",
    "            tag = tag.find_next(\"p\") # Now that we left the loop, we are looking for the first p, which would mean we entered the second section\n",
    "\n",
    "            if tag!= None:\n",
    "\n",
    "                if tag.find_parent('table') == None and tag.next!=\"\\n\" and tag.find_parent('blockquote') == None: # We added a condition on blockquote : we mustn't add quotes as a paragraph ! It's important since some wikipedia pages have an intro in which there is a quote.\n",
    "                    while tag.name == 'p':                    \n",
    "                        plot+=tag.text\n",
    "                        tag = tag.find_next_sibling()\n",
    "\n",
    "\n",
    "                else:\n",
    "                    while tag.text == \"\\n\" or tag.find_parent('table') != None or tag.find_parent('blockquote') != None:\n",
    "                        tag = tag.find_next(\"p\")\n",
    "\n",
    "\n",
    "                    while tag.name == 'p':\n",
    "                        plot+=tag.text\n",
    "                        tag = tag.find_next_sibling()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "\n",
    "        if intro != \"\":\n",
    "            intro = intro.replace(\"\\n\", \"\")\n",
    "            tot.append(intro)\n",
    "        else :\n",
    "            tot.append(\"NA\") # If we didn't find an intro, we simply write NA\n",
    "\n",
    "        if plot != \"\":\n",
    "            plot = plot.replace(\"\\n\", \"\")\n",
    "            tot.append(plot)\n",
    "        else :\n",
    "            tot.append(\"NA\")  # If we didn't find a plot, we simply write NA\n",
    "\n",
    "\n",
    "        # We now want to get the elements from the infobox\n",
    "\n",
    "        if right_table != None :\n",
    "            checkcat = 0 # This will enable us to check if a category is linked to some text. It is really important since, sometimes, an image is put instead\n",
    "            for row in right_table.findAll(\"tr\"):\n",
    "                cells = row.findAll('td')\n",
    "                states=row.findAll('th') #To store second column data\n",
    "                col = MyHTMLParser()\n",
    "                val = MyHTMLParser()\n",
    "                if len(states)>=1:\n",
    "                    col.feed(str(states[0]))\n",
    "                    a = col.return_info()\n",
    "                    if len(a)>=1:\n",
    "                        A.append(a[0])\n",
    "                    if len(cells)>=1:\n",
    "                        val.feed(str(cells[0]))\n",
    "                        b = val.return_info()\n",
    "                        if len(b)>=1:\n",
    "                            B.append(b)\n",
    "                        else:\n",
    "                            if checkcat != 0: # We want to keep the first element of the infobox, which is the title of the film\n",
    "                                A.remove(a[0])\n",
    "                    else:\n",
    "                        if checkcat != 0: # We want to keep the first element of the infobox, which is the title of the film\n",
    "                            A.remove(a[0])\n",
    "                checkcat+=1 \n",
    "\n",
    "\n",
    "            tot.append(A[0]) # We first append the title of the film\n",
    "\n",
    "            for i in range(len(att)):\n",
    "                if att[i] in A[1:]: # We check if we have each piece of information in A and B. If not, we will write \"NA\".\n",
    "                    ind = A.index(att[i])\n",
    "                    if A[ind] == \"Release date\":\n",
    "                        if len(B[ind-1])>1:\n",
    "                            date = ''.join(B[ind-1][2:len(B[ind-1])-1]) # We join all the dates (American date and European date for instance)\n",
    "                            date = date.replace(\"\\n\",\" \")\n",
    "                            tot.append(date)\n",
    "                        else:\n",
    "                            tot.append(B[ind-1][0])\n",
    "                    else:\n",
    "                        text = \" \".join(B[ind-1])\n",
    "                        text = text.replace(\"\\n\", \"\")\n",
    "                        tot.append(text)\n",
    "                else:\n",
    "                    tot.append(\"NA\")\n",
    "        else :\n",
    "            tot = tot + [\"NA\"]*11 # If we haven't found the infobox, we just append 11 times \"NA\", as asked on the instructions.\n",
    "\n",
    "\n",
    "        nametsv = 'C:/Users/chari/HW3ADMGR12/tsv1/urls1output'+str(k)+\".tsv\"\n",
    "        with open(nametsv, 'w', newline='',encoding = 'utf-8') as f_output:\n",
    "            tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "            tsv_output.writerow(tot)   \n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked into the problems that occurred and it resulted on a big amount of exceptions. If we hadn't put these exceptions, we would have got all the tsv files but with FALSE content ! For instance, we could have had many documents with a simple \"\\n\" as an intro ! We considered that we could sacrifice a few exceptions (3 for movies1) for the sake of having valid content in the tsv files.\n",
    "\n",
    "Please note that we retrieve the release date from index 2 (with the code ''.join(B[ind-1][2:len(B[ind-1])-1])) in order just to keep the (year/month/day) format. BUT, if there are multiple dates, this won't be true anymore. Anyway, this isn't a problem since it will just also write the litteral version of the date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided not to take the pages where we could find the table \"disambiguous\", which states that the page doesn't refer to a movie but to a set of wikipedia pages with similar names.\n",
    "\n",
    "The same process was repeated by the other members of the group on their folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the search engine, we had to \"clean\" the tsv files. First we removed the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to keep the URLs of the webpages we downloaded. Let us set the URLs of the movies which didn't have a wikipedia page to None. The following code is done for the folder movies1. The same process has been reproduced for the other folders.\n",
    "\n",
    "We could have done it in the previous block of code. Still, we thought that it would be more interesting to divide the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1 = [] # This list will store the index of all the existant tsv files\n",
    "\n",
    "for k in range (1,10001):\n",
    "    name = \"C:/Users/chari/HW3ADMGR12/tsv1/urls1output\" + str(k) + \".tsv\"\n",
    "    try:\n",
    "        open(name,'r')\n",
    "        valid1.append(k)\n",
    "    except Exception :\n",
    "        urls1[k-1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid2 = [] # This list will store the index of all the existant tsv files\n",
    "\n",
    "for k in range (1,10001):\n",
    "    name = \"C:/Users/chari/HW3ADMGR12/tsv2/output\" + str(k) + \".tsv\"\n",
    "    try:\n",
    "        open(name,'r')\n",
    "        valid2.append(k)\n",
    "    except Exception :\n",
    "        urls2[k-1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid3 = [] # This list will store the index of all the existant tsv files\n",
    "\n",
    "for k in range (1,10001):\n",
    "    name = \"C:/Users/chari/HW3ADMGR12/tsv3/urls3output\" + str(k) + \".tsv\"\n",
    "    try:\n",
    "        open(name,'r')\n",
    "        valid3.append(k)\n",
    "    except Exception :\n",
    "        urls3[k-1] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to use the links afterwards, especially for the search engines. In order not to run the previous code too many times, we create files to store all the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = 'C:/Users/chari/HW3ADMGR12/links1.json'\n",
    "links2 = 'C:/Users/chari/HW3ADMGR12/links2.json'\n",
    "links3 = 'C:/Users/chari/HW3ADMGR12/links3.json'\n",
    "val1 = 'C:/Users/chari/HW3ADMGR12/valid1.json'\n",
    "val2 = 'C:/Users/chari/HW3ADMGR12/valid2.json'\n",
    "val3 = 'C:/Users/chari/HW3ADMGR12/valid3.json'\n",
    "\n",
    "with open(links1, 'w') as f:\n",
    "    json.dump(urls1, f)\n",
    "    \n",
    "with open(links2, 'w') as f:\n",
    "    json.dump(urls2, f)\n",
    "\n",
    "with open(links3, 'w') as f:\n",
    "    json.dump(urls3, f)\n",
    "    \n",
    "with open(val1, 'w') as f:\n",
    "    json.dump(valid1, f)\n",
    "\n",
    "with open(val2, 'w') as f:\n",
    "    json.dump(valid2, f)\n",
    "    \n",
    "with open(val3, 'w') as f:\n",
    "    json.dump(valid3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now do not need to run all the code above to get the URLs. We can simply call the documents we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = \"C:/Users/chari/HW3ADMGR12/links1.json\"\n",
    "links2 = \"C:/Users/chari/HW3ADMGR12/links2.json\"\n",
    "links3 = \"C:/Users/chari/HW3ADMGR12/links3.json\"\n",
    "val1 = 'C:/Users/chari/HW3ADMGR12/valid1.json'\n",
    "val2 = 'C:/Users/chari/HW3ADMGR12/valid2.json'\n",
    "val3 = 'C:/Users/chari/HW3ADMGR12/valid3.json'\n",
    "\n",
    "with open(links1) as json_file:\n",
    "    urls1 = json.load(json_file)\n",
    "\n",
    "with open(links2) as json_file:\n",
    "    urls2 = json.load(json_file)\n",
    "    \n",
    "with open(links3) as json_file:\n",
    "    urls3 = json.load(json_file)\n",
    "    \n",
    "with open(val1) as json_file:\n",
    "    valid1 = json.load(json_file)\n",
    "\n",
    "with open(val2) as json_file:\n",
    "    valid2 = json.load(json_file)\n",
    "    \n",
    "with open(val3) as json_file:\n",
    "    valid3 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only kept the URLs of the valid webpages, we can start to create the vocabulary. Dictionaries are used there in order to link a word to all the documents in which it can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "dicti = {} # This dictionary will enable us to fill the index dictionary (for the Inverted index)\n",
    "index = {} # This dictionary will contain the \"patterns\" of words as keys and the name of the documents in which they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a function to be able to create the vocabulary and, afterwards, the index. It takes information from each folder (movies1, 2 or 3) and returns the updated version of the dictionary and a dictionary we will use to create the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creationind(folder,valid,tsv,dicti,vocab):\n",
    "    for k in valid:\n",
    "\n",
    "        file = open(tsv + str(k) + \".tsv\", \"r\", encoding='utf-8')\n",
    "        data = list(file)[0]\n",
    "        data = data.split(\"\\t\")\n",
    "        data = data[1:3]\n",
    "        data = \" \".join(data)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        words = tokenizer.tokenize(data)\n",
    "\n",
    "        wordsFiltered = []\n",
    "\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        for w in words:\n",
    "            if w.lower() not in stopWords:  # We used the lower() function because some stopwords can be written with an uppercase in the tsv files.\n",
    "                wordsFiltered.append(w)\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        for w in wordsFiltered:\n",
    "\n",
    "            if ps.stem(w) not in vocab:\n",
    "                vocab.append(ps.stem(w))\n",
    "                dicti[ps.stem(w)] = [\"document\"+str(folder)+\"_\" + str(k)]\n",
    "            else:\n",
    "                if \"document\"+str(folder)+\"_\" + str(k) not in dicti[ps.stem(w)]:\n",
    "                    dicti[ps.stem(w)].append(\"document\"+str(folder)+\"_\" + str(k))\n",
    "    return(dicti,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv1 = \"C:/Users/chari/HW3ADMGR12/tsv1/urls1output\"\n",
    "tsv2 = \"C:/Users/chari/HW3ADMGR12/tsv2/output\"\n",
    "tsv3 = \"C:/Users/chari/HW3ADMGR12/tsv3/urls3output\"\n",
    "\n",
    "\n",
    "with open(links1) as json_file:\n",
    "    urls1 = json.load(json_file)\n",
    "\n",
    "with open(links2) as json_file:\n",
    "    urls2 = json.load(json_file)\n",
    "\n",
    "with open(links3) as json_file:\n",
    "    urls3 = json.load(json_file)\n",
    "\n",
    "with open(val1) as json_file:\n",
    "    valid1 = json.load(json_file)\n",
    "\n",
    "with open(val2) as json_file:\n",
    "    valid2 = json.load(json_file)\n",
    "\n",
    "with open(val3) as json_file:\n",
    "    valid3 = json.load(json_file)\n",
    "    \n",
    "folder = 1\n",
    "\n",
    "allinfos = creationind(folder,valid1,tsv1,dicti,vocab)\n",
    "dicti = allinfos[0]\n",
    "vocab = allinfos[1]\n",
    "\n",
    "folder = 2\n",
    "\n",
    "allinfos = creationind(folder,valid2,tsv2,dicti,vocab)\n",
    "dicti = allinfos[0]\n",
    "vocab = allinfos[1]\n",
    "\n",
    "folder = 3\n",
    "\n",
    "allinfos = creationind(folder,valid3,tsv3,dicti,vocab)\n",
    "dicti = allinfos[0]\n",
    "vocab = allinfos[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, we created a function to handle the creation of the vocabulary. It makes the code more readable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the words we could find in the intro and the plot of each of the 30000 movies. We can now create the inverted index. We just go over the list *vocab* and append to a new dictionary, \"index\", the index of each word and the list of documents in which we can find the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = {} # We will store the vocabulary into a json file, thanks to a dictionary\n",
    "\n",
    "    \n",
    "# Finally, we got the inverted index and the vocabulary.\n",
    "\n",
    "for k in range (len(vocab)):\n",
    "    voc[vocab[k]] = k\n",
    "    index[k] = dicti[vocab[k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the vocabulary file and the index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = \"C:/Users/chari/HW3ADMGR12/vocabulary.json\"\n",
    "\n",
    "with open(vocabulary, 'w') as f:\n",
    "    json.dump(voc, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexn = 'C:/Users/chari/HW3ADMGR12/index.json'\n",
    "\n",
    "with open(indexn, 'w') as f:\n",
    "    json.dump(index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we got the files we need, we can compute the first search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = \"C:/Users/chari/HW3ADMGR12/vocabulary.json\"\n",
    "indexn = 'C:/Users/chari/HW3ADMGR12/index.json'\n",
    "\n",
    "with open(indexn) as json_file:\n",
    "    index = json.load(json_file)\n",
    "    \n",
    "with open(vocabulary) as json_file:\n",
    "    voc = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order not to run again the code to create the vocabulary and the inverted index, we simply load the files we created. Even though the index is a bit \"heavy\" (92.9 MB), the code is run fastly. \n",
    "\n",
    "It could have got slower if the vocabulary was bigger; if we consider more wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = \"C:/Users/chari/HW3ADMGR12/links1.json\"\n",
    "links2 = \"C:/Users/chari/HW3ADMGR12/links2.json\"\n",
    "links3 = \"C:/Users/chari/HW3ADMGR12/links3.json\"\n",
    "val1 = 'C:/Users/chari/HW3ADMGR12/valid1.json'\n",
    "val2 = 'C:/Users/chari/HW3ADMGR12/valid2.json'\n",
    "val3 = 'C:/Users/chari/HW3ADMGR12/valid3.json'\n",
    "\n",
    "with open(links1) as json_file:\n",
    "    urls1 = json.load(json_file)\n",
    "\n",
    "with open(links2) as json_file:\n",
    "    urls2 = json.load(json_file)\n",
    "    \n",
    "with open(links3) as json_file:\n",
    "    urls3 = json.load(json_file)\n",
    "    \n",
    "with open(val1) as json_file:\n",
    "    valid1 = json.load(json_file)\n",
    "\n",
    "with open(val2) as json_file:\n",
    "    valid2 = json.load(json_file)\n",
    "    \n",
    "with open(val3) as json_file:\n",
    "    valid3 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney movies 2019\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the query is entered, we split it and apply the same steps we applied to the words of the tsv files. In that way, we would pre-process the query to get the most accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\"The Chronicles of Narnia: Prince Caspian is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>\"Frozen is a 2013 American 3D computer-animate...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>Marvel's The Avengers[6] (classified under the...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>Cars 3 is a 2017 American 3D computer-animated...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0  The Chronicles of Narnia: Prince Caspian   \n",
       "1                                    Frozen   \n",
       "2                              The Avengers   \n",
       "3                                    Cars 3   \n",
       "4                                Thumbelina   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  \"The Chronicles of Narnia: Prince Caspian is a...   \n",
       "1  \"Frozen is a 2013 American 3D computer-animate...   \n",
       "2  Marvel's The Avengers[6] (classified under the...   \n",
       "3  Cars 3 is a 2017 American 3D computer-animated...   \n",
       "4  Thumbelina (also known as Hans Christian Ander...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://en.wikipedia.org/wiki/The_Chronicles_o...  \n",
       "1   https://en.wikipedia.org/wiki/Frozen_(2013_film)  \n",
       "2  https://en.wikipedia.org/wiki/The_Avengers_(20...  \n",
       "3               https://en.wikipedia.org/wiki/Cars_3  \n",
       "4  https://en.wikipedia.org/wiki/Thumbelina_(1994...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allquery = query.split()\n",
    "\n",
    "\n",
    "# ---------- We have to apply the same process (stemming,removing stopwords...) on the words of the query :\n",
    "\n",
    "wordsFiltered = []\n",
    "\n",
    "allquery = query.split()    \n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "for w in allquery:\n",
    "    if w.lower() not in stopWords: # We used the lower() function because some stopwords can be written with an uppercase in the tsv files.\n",
    "        wordsFiltered.append(w.lower())\n",
    "\n",
    "allquery = []\n",
    "ps = PorterStemmer()\n",
    "for w in wordsFiltered:\n",
    "    allquery.append(ps.stem(w))\n",
    "\n",
    "# -----------------------------------------------------------        \n",
    "\n",
    "subresult = [] # This list will contain each result of the query, before storing it in the dataframe \"result\".\n",
    "d = []\n",
    "\n",
    "if all(elem in voc.keys() for elem in allquery):\n",
    "    \n",
    "    for i in range(len(allquery)):\n",
    "    \n",
    "        d.append(index[str(list(voc.keys()).index(allquery[i]))])\n",
    "       \n",
    "    alldocs = list(set(d[0]).intersection(*d)) # We get the intersection of all the documents that contain ALL the words of the query.\n",
    "\n",
    "    \n",
    "    for k in range(len(alldocs)):\n",
    "        \n",
    "        iddoc = int(list(set(d[0]).intersection(*d))[k].split(\"_\")[1]) # This is the id of the document. We can now retrieve the info from the targeted movie\n",
    "        \n",
    "        if list(set(d[0]).intersection(*d))[k].split(\"_\")[0] == 'document1' : # If the document in which we find the word has that name (+\"_k\"), we retrieve the tsv files from the set of movies1\n",
    "            file = open(\"C:/Users/chari/HW3ADMGR12/tsv1/urls1output\"+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls1[iddoc-1]\n",
    "            \n",
    "        elif list(set(d[0]).intersection(*d))[k].split(\"_\")[0] == 'document2':\n",
    "            file = open(\"C:/Users/chari/HW3ADMGR12/tsv2/output\"+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls2[iddoc-1]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            file = open(\"C:/Users/chari/HW3ADMGR12/tsv3/urls3output\"+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls3[iddoc-1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        data = list(file)[0]\n",
    "        data = data.split(\"\\t\")\n",
    "        subresult.append([data[0],data[1],url])\n",
    "        \n",
    "        \n",
    "# Let us also create a dataframe to present the result of the query\n",
    "        \n",
    "result = pd.DataFrame(subresult,columns = [\"Title\",\"Intro\",\"Url\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we recovered the names of the documents. After that, we considered each name of the document in the resut of the query. We retrieved the id of the document, so that we could feed the dataframe \"result\" with the title, intro and url of **each film which is related to all of the words in the query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will get a bit more in detail by calculating the tf-idf of each word, in order to build another search engine.\n",
    "\n",
    "### Then we're going to calculate similarity for each parameter and at the end calculate a weighted sum over all of them.\n",
    "\n",
    "#### So at first, we use search engine of part 2.2 to get the results of the text query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.json\") as json_file:\n",
    "    all_words = json.load(json_file)\n",
    "with open(\"index.json\") as json_file:\n",
    "    indices = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the following code twice. The first one is useful to build the inverted index and the second will be used during each search made. We did not see the point in storing the idf dictionary as the running time is short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 29501 # total number of documents\n",
    "#compute IDF\n",
    "idfDict = {}\n",
    "for word in all_words.keys():\n",
    "    word_index = all_words[word]\n",
    "    df = len(indices[str(word_index)])\n",
    "    idfDict[word] = math.log(N/float(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {k: [] for k in indices.keys()}\n",
    "\n",
    "def update_inverted_index(path, folder):\n",
    "    for k in range(1, 10001): \n",
    "        try:\n",
    "            file = open(path+str(k)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            data = list(file)[0]\n",
    "            data = data.split(\"\\t\")\n",
    "            data = data[:3]\n",
    "            data = \" \".join(data)\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            words = tokenizer.tokenize(data)\n",
    "            wordsFiltered = []\n",
    "            stopWords = set(stopwords.words('english'))\n",
    "\n",
    "            for w in words:\n",
    "                if w.lower() not in stopWords: # We used the lower() function because some stopwords can be written with an uppercase in the tsv files.\n",
    "                    wordsFiltered.append(w)\n",
    "\n",
    "            word_dict = {}\n",
    "            ps = PorterStemmer() \n",
    "            for w in wordsFiltered:\n",
    "                word_st = ps.stem(w)\n",
    "                if  word_st not in word_dict:\n",
    "                    word_dict[word_st] = 1\n",
    "                else:\n",
    "                    word_dict[word_st] += 1\n",
    "\n",
    "            #compute tf and tfidf\n",
    "            for word, count in word_dict.items():\n",
    "                tf = count/float(len(wordsFiltered))\n",
    "                tfidf = tf * idfDict[word]\n",
    "                word_index = all_words[word]\n",
    "                #update the inverted index\n",
    "                inverted_index[str(word_index)].append(('Document' + str(folder) + \"_\" + str(k), tfidf))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "update_inverted_index('C:/Users/chari/HW3ADMGR12/tsv1/urls1output', 1)\n",
    "update_inverted_index('C:/Users/chari/HW3ADMGR12/tsv2/output', 2)\n",
    "update_inverted_index('C:/Users/chari/HW3ADMGR12/tsv3/urls3output', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index.json', 'w') as f:\n",
    "    json.dump(inverted_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the inverted index (through a json file), we can focus on the search engine. As we do not want to create the index at each serch, we load it in the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = 'C:/Users/chari/HW3ADMGR12/inverted_index.json'\n",
    "all_words = 'C:/Users/chari/HW3ADMGR12/vocabulary.json'\n",
    "indices = 'C:/Users/chari/HW3ADMGR12/index.json'\n",
    "\n",
    "\n",
    "with open(inverted_index) as json_file:\n",
    "    inverted_index = json.load(json_file)\n",
    "\n",
    "with open(all_words) as json_file:\n",
    "    all_words = json.load(json_file)\n",
    "    \n",
    "with open(indices) as json_file:\n",
    "    indices = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 29501 # total number of documents\n",
    "#compute IDF\n",
    "idfDict = {}\n",
    "for word in all_words.keys():\n",
    "    word_index = all_words[word]\n",
    "    df = len(indices[str(word_index)])\n",
    "    idfDict[word] = math.log(N/float(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney movies 2019\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have inverted_index we can handle the queries\n",
    "\n",
    "allquery = query.split()\n",
    "# ---------- We have to apply the same process (stemming,removing stopwords...) on the words of the query :\n",
    "wordsFiltered = []\n",
    "allquery = query.split()    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "for w in allquery:\n",
    "    if w.lower() not in stopWords: # We used the lower() function because some stopwords can be written with an uppercase in the tsv files.\n",
    "        wordsFiltered.append(w.lower()) \n",
    "allquery = []\n",
    "ps = PorterStemmer()\n",
    "for w in wordsFiltered:\n",
    "    allquery.append(ps.stem(w))\n",
    "    \n",
    "    \n",
    "subresult = [] # This list will contain each result of the query\n",
    "d = []\n",
    "\n",
    "if all(elem in all_words.keys() for elem in allquery):    \n",
    "    for w in allquery:   \n",
    "        d.append(indices[str(all_words[w])]) \n",
    "    alldocs = list(set(d[0]).intersection(*d))\n",
    "    \n",
    "    for k in range(len(alldocs)):        \n",
    "        name = alldocs[k].split(\"_\")\n",
    "        iddoc = int(name[1]) # This is the id of the document. \n",
    "        folder = int(name[0][-1])\n",
    "        #We can now retrieve the info from the targeted movie\n",
    "        if folder == 1:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv1/urls1output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls1[iddoc-1]\n",
    "        elif folder == 2:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv2/output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls2[iddoc-1]\n",
    "        elif folder == 3:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv3/urls3output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls3[iddoc-1]\n",
    "        data = list(file)[0]\n",
    "        data = data.split(\"\\t\")\n",
    "        subresult.append([data[0],data[1],url, iddoc, folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>0.929615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>Marvel's The Avengers[6] (classified under the...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>0.929615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>Cars 3 is a 2017 American 3D computer-animated...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>0.878929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>\"Frozen is a 2013 American 3D computer-animate...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>0.810984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\"The Chronicles of Narnia: Prince Caspian is a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>0.771815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0                                Thumbelina   \n",
       "1                              The Avengers   \n",
       "2                                    Cars 3   \n",
       "3                                    Frozen   \n",
       "4  The Chronicles of Narnia: Prince Caspian   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  Thumbelina (also known as Hans Christian Ander...   \n",
       "1  Marvel's The Avengers[6] (classified under the...   \n",
       "2  Cars 3 is a 2017 American 3D computer-animated...   \n",
       "3  \"Frozen is a 2013 American 3D computer-animate...   \n",
       "4  \"The Chronicles of Narnia: Prince Caspian is a...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://en.wikipedia.org/wiki/Thumbelina_(1994...    0.929615  \n",
       "1  https://en.wikipedia.org/wiki/The_Avengers_(20...    0.929615  \n",
       "2               https://en.wikipedia.org/wiki/Cars_3    0.878929  \n",
       "3   https://en.wikipedia.org/wiki/Frozen_(2013_film)    0.810984  \n",
       "4  https://en.wikipedia.org/wiki/The_Chronicles_o...    0.771815  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we compute tf-idf for each word in the query\n",
    "if len(subresult) != 0:\n",
    "    word_dict = {}\n",
    "    query_tfidf = [] #vector to store all tf-idf scores\n",
    "    #compute tf-idf for each word in the query\n",
    "    for w in allquery: \n",
    "        if  w not in word_dict:\n",
    "            word_dict[w] = 1\n",
    "        else:\n",
    "            word_dict[w] += 1\n",
    "    for w in allquery:\n",
    "        count = word_dict[w]\n",
    "        tf = count/float(len(allquery))\n",
    "        tfidf = tf * idfDict[word]\n",
    "        query_tfidf += [tfidf]\n",
    "\n",
    "        \n",
    "        \n",
    "#now we need to create the same vector for each result\n",
    "results = []\n",
    "for item in subresult:\n",
    "    word_dict = {}\n",
    "    tfidf_vector = []\n",
    "    for w in allquery:\n",
    "        for elem in inverted_index[str(all_words[w])]:\n",
    "            if elem[0] == 'Document' + str(item[4]) + '_' + str(item[3]):\n",
    "                tfidf_vector.append(elem[1])\n",
    "    #now that we have the tf-idf vector we can calculate similarity\n",
    "    similarity = cosine_similarity([query_tfidf], [tfidf_vector])[0][0]\n",
    "    results.append((similarity, item))\n",
    "k = 10 #how many results are we going to show\n",
    "results = heapq.nlargest(10, results)\n",
    "final_results = []\n",
    "for item in results:\n",
    "    final_results.append([item[1][0], item[1][1], item[1][2], item[0]])\n",
    "result = pd.DataFrame(final_results,columns = [\"Title\",\"Intro\",\"Url\", \"Similarity\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result respects the instruction of showing k results (10, here) if we have k results out of the query. If not, we simply print all the results we find. Here, as the query *Disney movies 2019* only gives 5 results, we display them all. Also, we can see that the movie webpage that has most similarity with the query is **Thumbelina**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We decided to take several additional information into account. After entering the \"normal\" query (as in part 2.1), the user will be able to specify his request, by entering the release year of the movies, the starring actors/acctresses..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to let the user enter another information that would lead us to rank the results of the query according it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in order not to scroll up, we load the URLs, the vocabulary and the index in the following block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = \"C:/Users/chari/HW3ADMGR12/links1.json\"\n",
    "links2 = \"C:/Users/chari/HW3ADMGR12/links2.json\"\n",
    "links3 = \"C:/Users/chari/HW3ADMGR12/links3.json\"\n",
    "\n",
    "with open(links1) as json_file:\n",
    "    urls1 = json.load(json_file)\n",
    "\n",
    "with open(links2) as json_file:\n",
    "    urls2 = json.load(json_file)\n",
    "    \n",
    "with open(links3) as json_file:\n",
    "    urls3 = json.load(json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.json\") as json_file:\n",
    "    all_words = json.load(json_file)\n",
    "with open(\"index.json\") as json_file:\n",
    "    indices = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now enter the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "godfather\n"
     ]
    }
   ],
   "source": [
    "#now that we have inverted_index we can handle the queries\n",
    "query = input()\n",
    "allquery = query.split()\n",
    "# ---------- We have to apply the same process (stemming,removing stopwords...) on the words of the query :\n",
    "wordsFiltered = []\n",
    "allquery = query.split()    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "for w in allquery:\n",
    "    if w.lower() not in stopWords: # We used the lower() function because some stopwords can be written with an uppercase in the tsv files.\n",
    "        wordsFiltered.append(w.lower()) \n",
    "allquery = []\n",
    "ps = PorterStemmer()\n",
    "for w in wordsFiltered:\n",
    "    allquery.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subresult = [] # This list will contain each result of the query\n",
    "d = []\n",
    "\n",
    "if all(elem in all_words.keys() for elem in allquery):    \n",
    "    for w in allquery:   \n",
    "        d.append(indices[str(all_words[w])]) \n",
    "    alldocs = list(set(d[0]).intersection(*d))\n",
    "    \n",
    "    for k in range(len(alldocs)):        \n",
    "        name = alldocs[k].split(\"_\")\n",
    "        iddoc = int(name[1]) # This is the id of the document. \n",
    "        folder = int(name[0][-1])\n",
    "        #We can now retrieve the info from the targeted movie\n",
    "        if folder == 1:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv1/urls1output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls1[iddoc-1]\n",
    "        elif folder == 2:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv2/output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls2[iddoc-1]\n",
    "        elif folder == 3:\n",
    "            file = open('C:/Users/chari/HW3ADMGR12/tsv3/urls3output'+str(iddoc)+\".tsv\",\"r\",encoding='utf-8')\n",
    "            url = urls3[iddoc-1]\n",
    "        data = list(file)[0]\n",
    "        data = data.split(\"\\t\")\n",
    "        subresult.append([data,url, iddoc, folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code enables the user to choose a category of the data stored in the tsv files.\n",
    "#### He can then enter the name of an actor, of a musician or also the running time of a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of parameter you wish to add:\n",
      "1. Film Name\n",
      "2. Director\n",
      "3. Producer\n",
      "4. Writer\n",
      "5. Starring\n",
      "6. Music\n",
      "7. Release Date\n",
      "8. Runtime\n",
      "9. Country\n",
      "10. Language\n",
      "11. Budget\n",
      "Enter 0 if you're done.\n",
      "2\n",
      "Enter Director's name:\n",
      "coppola\n",
      "Enter the number of parameter you wish to add:\n",
      "1. Film Name\n",
      "2. Director\n",
      "3. Producer\n",
      "4. Writer\n",
      "5. Starring\n",
      "6. Music\n",
      "7. Release Date\n",
      "8. Runtime\n",
      "9. Country\n",
      "10. Language\n",
      "11. Budget\n",
      "Enter 0 if you're done.\n",
      "5\n",
      "Enter Star's name:\n",
      "brando\n",
      "Enter the number of parameter you wish to add:\n",
      "1. Film Name\n",
      "2. Director\n",
      "3. Producer\n",
      "4. Writer\n",
      "5. Starring\n",
      "6. Music\n",
      "7. Release Date\n",
      "8. Runtime\n",
      "9. Country\n",
      "10. Language\n",
      "11. Budget\n",
      "Enter 0 if you're done.\n",
      "6\n",
      "Enter Musician's name:\n",
      "rota\n",
      "Enter the number of parameter you wish to add:\n",
      "1. Film Name\n",
      "2. Director\n",
      "3. Producer\n",
      "4. Writer\n",
      "5. Starring\n",
      "6. Music\n",
      "7. Release Date\n",
      "8. Runtime\n",
      "9. Country\n",
      "10. Language\n",
      "11. Budget\n",
      "Enter 0 if you're done.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "query = {'text': allquery,'name':[], 'director':[], 'producer':[], 'writer':[], 'starring':[],\n",
    "         'music':[], 'release_date':[], 'runtime':[], 'country':[], 'language':[], 'budget':[]}\n",
    "number = 1\n",
    "while number != 0:\n",
    "    number = int(input(\"Enter the number of parameter you wish to add:\" + \n",
    "              \"\\n1. Film Name\\n2. Director\\n3. Producer\\n4. Writer\\n5. \" + \n",
    "              \"Starring\\n6. Music\\n7. Release Date\\n8. Runtime\\n9. \" + \n",
    "               \"Country\\n10. Language\\n11. Budget\\nEnter 0 if you're done.\\n\"))\n",
    "    if number == 1:\n",
    "        name = input(\"Enter Film Name:\\n\")\n",
    "        query['name'] += [name]\n",
    "    elif number == 2:\n",
    "        director = input(\"Enter Director's name:\\n\")\n",
    "        query['director'] += [director]\n",
    "    elif number == 3:\n",
    "        Producer = input(\"Enter Producer's name:\\n\")\n",
    "        query['producer'] += [Producer]\n",
    "    elif number == 4:\n",
    "        Writer = input(\"Enter Writer's name:\\n\")\n",
    "        query['writer'] += [Writer]\n",
    "    elif number == 5:\n",
    "        Starring = input(\"Enter Star's name:\\n\")\n",
    "        query['starring'] += [Starring]\n",
    "    elif number == 6:\n",
    "        Music = input(\"Enter Musician's name:\\n\")\n",
    "        query['music'] += [Music]\n",
    "    elif number == 7:\n",
    "        release_date = input(\"Enter Release Year:\\n\")\n",
    "        query['release_date'] += [release_date]\n",
    "    elif number == 8:\n",
    "        runtime = input(\"Enter Runtime in minutes:\\n\")\n",
    "        query['runtime'] += [runtime]\n",
    "    elif number == 9:\n",
    "        Country = input(\"Enter Country:\\n\")\n",
    "        query['country'] += [Country]\n",
    "    elif number == 10:\n",
    "        Language = input(\"Enter Language:\\n\")\n",
    "        query['language'] += [Language]\n",
    "    elif number == 11:\n",
    "        Budget = input(\"Enter Budget:\\n\")\n",
    "        query['budget'] += [Budget]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function1(query, info):\n",
    "    if len(query) == 0 or info == 'NA': \n",
    "        similarity = 0.5 # If the user doesn't specify any parameter we consider similarity .5\n",
    "    else:\n",
    "        cnt = 0\n",
    "        for item in query:\n",
    "            if item.lower() in info.lower():\n",
    "                cnt += 1\n",
    "        similarity = cnt/len(query)\n",
    "    return similarity\n",
    "    \n",
    "def scoring_function2(difference, normalization):\n",
    "    #for quantitative parameters we're going to use a negetive exponential function\n",
    "    #to calculate the similarity between the query and the actual value\n",
    "    #the function is similarity = exp (-(difference)/(normalization))\n",
    "    #when the release year is equal to the query the difference is 0 and similarity is 1\n",
    "    return math.exp(-(difference/normalization))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(subresult):\n",
    "    results = []\n",
    "    for i in range(len(subresult)):\n",
    "        similarity = {} # a number between 0 and 1\n",
    "        similarity['name'] = scoring_function1(query['name'], subresult[i][0][3])\n",
    "        similarity['director'] = scoring_function1(query['director'], subresult[i][0][4])\n",
    "        similarity['producer'] = scoring_function1(query['producer'], subresult[i][0][5])\n",
    "        similarity['writer'] = scoring_function1(query['writer'], subresult[i][0][6])\n",
    "        similarity['starring'] = scoring_function1(query['starring'], subresult[i][0][7])\n",
    "        similarity['music'] = scoring_function1(query['music'], subresult[i][0][8])\n",
    "        similarity['country'] = scoring_function1(query['country'], subresult[i][0][11])\n",
    "        similarity['language'] = scoring_function1(query['language'], subresult[i][0][12])\n",
    "        \n",
    "        if len(query['release_date']) == 0  or subresult[i][0][9] == 'NA': \n",
    "            similarity['release_date'] = 0.5\n",
    "        else: \n",
    "            year = re.search(r'\\d{4}', subresult[i][0][9]) \n",
    "            if year != None:\n",
    "                year = int(year.group(0))\n",
    "                similarity['release_date'] = scoring_function2(abs(year - int(query['release_date'][0])), 10)\n",
    "            else:\n",
    "                similarity['release_date'] = 0.5\n",
    "\n",
    "        if len(query['runtime']) == 0  or subresult[i][0][10] == 'NA': \n",
    "            similarity['runtime'] = 0.5\n",
    "        else: \n",
    "            runtime = re.search(r'\\d+', subresult[i][0][10]) \n",
    "            if year != None:\n",
    "                runtime = int(runtime.group(0))\n",
    "                similarity['runtime'] = scoring_function2(abs(runtime - int(query['runtime'][0])), 25)\n",
    "            else:\n",
    "                similarity['runtime'] = 0.5\n",
    "\n",
    "    #     print(subresult[i][0][3])\n",
    "    #     print(similarity)\n",
    "        final_value = 0\n",
    "        for key, value in similarity.items():\n",
    "            if key == 'name':\n",
    "                final_value += 2*value\n",
    "            else:\n",
    "                final_value += value\n",
    "        results.append((final_value/12, subresult[i]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>\"The Godfather is a 1972 American crime film d...</td>\n",
       "      <td>\"In 1945, at his daughter Connie's wedding to ...</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>\"The Godfather Part II is a 1974 American crim...</td>\n",
       "      <td>In 1958, during his son's First Communion part...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eadweard (film)</td>\n",
       "      <td>Eadweard is a 2015 Canadian drama film written...</td>\n",
       "      <td>A psychological drama centered around world-fa...</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Rain People</td>\n",
       "      <td>The Rain People is a 1969 film directed by Fra...</td>\n",
       "      <td>Housewife Natalie Ravenna (Shirley Knight) dec...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather Part III</td>\n",
       "      <td>The Godfather Part III is a 1990 American crim...</td>\n",
       "      <td>In 1979, Michael Corleone, approaching 60, is ...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Freshman</td>\n",
       "      <td>The Freshman is a 1990 American crime comedy f...</td>\n",
       "      <td>\"Clark Kellogg (Matthew Broderick) leaves his ...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Conversation</td>\n",
       "      <td>\"The Conversation is a 1974 American mystery t...</td>\n",
       "      <td>\"Harry Caul is a surveillance expert who runs ...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Z Storm</td>\n",
       "      <td>Z Storm (Chinese: Z), is a 2014 Hong Kong cr...</td>\n",
       "      <td>This is a story about the biggest financial fr...</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Three Godfathers</td>\n",
       "      <td>Three Godfathers is a 1936 western film direct...</td>\n",
       "      <td>A week before Christmas, four bandits ride thr...</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Boat That Rocked</td>\n",
       "      <td>\"The Boat That Rocked (titled Pirate Radio in ...</td>\n",
       "      <td>\"In 1966, various pirate radio stations broadc...</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Title                                              Intro  \\\n",
       "0           The Godfather  \"The Godfather is a 1972 American crime film d...   \n",
       "1   The Godfather Part II  \"The Godfather Part II is a 1974 American crim...   \n",
       "2         Eadweard (film)  Eadweard is a 2015 Canadian drama film written...   \n",
       "3         The Rain People  The Rain People is a 1969 film directed by Fra...   \n",
       "4  The Godfather Part III  The Godfather Part III is a 1990 American crim...   \n",
       "5            The Freshman  The Freshman is a 1990 American crime comedy f...   \n",
       "6        The Conversation  \"The Conversation is a 1974 American mystery t...   \n",
       "7                 Z Storm  Z Storm (Chinese: Z), is a 2014 Hong Kong cr...   \n",
       "8        Three Godfathers  Three Godfathers is a 1936 western film direct...   \n",
       "9    The Boat That Rocked  \"The Boat That Rocked (titled Pirate Radio in ...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  \"In 1945, at his daughter Connie's wedding to ...    0.583333  \n",
       "1  In 1958, during his son's First Communion part...    0.500000  \n",
       "2  A psychological drama centered around world-fa...    0.458333  \n",
       "3  Housewife Natalie Ravenna (Shirley Knight) dec...    0.416667  \n",
       "4  In 1979, Michael Corleone, approaching 60, is ...    0.416667  \n",
       "5  \"Clark Kellogg (Matthew Broderick) leaves his ...    0.416667  \n",
       "6  \"Harry Caul is a surveillance expert who runs ...    0.416667  \n",
       "7  This is a story about the biggest financial fr...    0.375000  \n",
       "8  A week before Christmas, four bandits ride thr...    0.375000  \n",
       "9  \"In 1966, various pirate radio stations broadc...    0.375000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results(subresult)\n",
    "k = 10 #how many results are we going to show\n",
    "results = heapq.nlargest(10, results)\n",
    "final_results = []\n",
    "for item in results:\n",
    "    final_results.append([item[1][0][0], item[1][0][1], item[1][0][2], item[0]])\n",
    "result = pd.DataFrame(final_results,columns = [\"Title\",\"Intro\",\"Url\", \"Similarity\"])\n",
    "result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, the movie that is the nearest to the query is the first *Godfather*. It is logical since all the information entered are linked to this film (Nino Rota made the music, for instance). \n",
    "\n",
    "Our third search engine is more precise than the two previous ones. It enables us to give a detailed query to sharpen and improve the accuracy of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code of this part is stored in the file exercise_4.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use a matrix that allow us to count the length of the longest palindromic subsequence. In particular we'll use a dynamic way to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Palindromic(s):\n",
    "    l=len(s)\n",
    "    A=[]\n",
    "    # A is the matrix that represents the length of the longest palindromic subsequence in the interval [i,j] \n",
    "    # for now I initialize it at 0 (i!=j) and 1 (i==j) because at this very fist moment I'll consider only \n",
    "    # every single char (and obviously a char is palindromic with itself)\n",
    "    for i in range(l):\n",
    "        rowA=[]\n",
    "        for j in range(l):\n",
    "            if i==j:\n",
    "                rowA.append(1)\n",
    "            else:\n",
    "                rowA.append(0)\n",
    "        A.append(rowA)\n",
    " \n",
    "    # Now I have to fill the matrix with the subinterval lenght: note that the longest Pal length is \n",
    "    # in position A[0][l-1]: the last element of first row (and for this reason the return is this value)\n",
    "    for k in range(2, l+1):\n",
    "        for i in range(l-k+1):\n",
    "            j = i+k-1\n",
    "            # I used this print to check every comparison\n",
    "            #print(\"s[i],s[j] \", s[i],s[j], \" i \",i,\" j \", j)\n",
    "            if (s[i] == s[j]):\n",
    "                A[i][j] = A[i+1][j-1] + 2;\n",
    "            else:\n",
    "                A[i][j] = max(A[i][j-1], A[i+1][j])\n",
    " \n",
    "    return A[0][l-1]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have the function and we're able to check the longest  palindromic subsequence length of our string \"DATAMININGSAPIENZA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "s1=\"DATAMININGSAPIENZA\"\n",
    "lp=Palindromic(s1)\n",
    "print(lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: for clarity we specify that the element A[i][j] compares the i-th and the j-th char of the string; so the matrix is composed of the length value of the longest sequence up to that particular comparison (for this reason the final length is in position A[0][l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we have been able to discover, for some of us, how to download webpages and parse them. We had to deal with large amount of data. To be able to keep the information we were interested in, we had to take some decisions and make hypothesis, in particular while creating the TSV files (like being careful that we don't recover paragraphs from the infobox, for instance).\n",
    "\n",
    "As for the search engine, we saw that storing the whole vocabulary was useful, even though loading it can become slow when the vocabulary becomes too heavy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
